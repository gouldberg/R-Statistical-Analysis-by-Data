data(brains, package = "gamlss.mx")
str(brains)
car::some(brains)
data(brains, package = "gamlss.mx")
str(brains)
car::some(brains)
packages <- c("dplyr", "gamlss")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# data:  brains
#   - The average brain size and body weight were recorded for 28 species of land animals.
#   - Variables
#       - brain:  brain weight in g.
#       - body:  body weight in kg.
# ------------------------------------------------------------------------------
data(brains, package = "gamlss.mx")
str(brains)
car::some(brains)
# ------------------------------------------------------------------------------
# basics
# ------------------------------------------------------------------------------
car::densityPlot( ~ body, data = brains)
data(brains, package = "gamlss.mx")
str(brains)
car::some(brains)
# ------------------------------------------------------------------------------
# Data Exploration:  data transformation for "body"
# ------------------------------------------------------------------------------
# check density
car::densityPlot( ~ body, data = brains)
data(brains, package = "gamlss.mx")
install.packages("gamlss.mx", dep = T)
data(brains, package = "gamlss.mx")
str(brains)
car::some(brains)
# ------------------------------------------------------------------------------
# Data Exploration:  data transformation for "body"
# ------------------------------------------------------------------------------
# check density
car::densityPlot( ~ body, data = brains)
# transforming for symmetry
car::symbox(~ body, data = brains)
p1 <- car::powerTransform(body ~ 1, data = brains, family = "bcPower")
summary(p1)
# should not use lambda = 0
car::testTransform(p1, lambda = 0)
tmp <- brains %>% mutate(body2 = log(body))
car::densityPlot( ~ body2, data = tmp)
car::densityPlot( ~ brain, data = brains)
# transforming for symmetry
car::symbox(~ brain, data = brains)
# check Box-Cox power family transformation
# Rounded Pwr is the first value among {1, 0, -1, 0.5, 0.33, -0.5, -0.33, 2, -2} that is included in the confidence interval for lambda
# The test for the log transformation has a very large p-value, indicating that the log transformation is consistent with the data,
p1 <- car::powerTransform(brain ~ 1, data = brains, family = "bcPower")
summary(p1)
par(mfrow=c(1,1))
with(brains, plot(lbrain ~ lbody, ylab = "log brain", xlab = "log body"))
lines(lowess(brains$lbody, brains$lbrain), col = "blue", lwd = 1)
brains <- transform(brains, lbrain = log(brain), lbody= log(body))
# ------------------------------------------------------------------------------
# data exploration:  Y vs. continuous X
# ------------------------------------------------------------------------------
par(mfrow=c(1,1))
with(brains, plot(lbrain ~ lbody, ylab = "log brain", xlab = "log body"))
lines(lowess(brains$lbody, brains$lbrain), col = "blue", lwd = 1)
library(ggplot2)
gg <- ggplot(brains, aes(lbody, lbrain)) +
stat_smooth(method = "loess", size = 2, fill = "blue", alpha = 0.25) +
stat_smooth(method = "lm", color = "red", size = 1.25, se = FALSE) +
scale_y_log10(breaks = c(1, 2, 5, 10, 20)) +
labs(y = "exra", x = "nao")
gg
br.1 <- gamlss(lbrain ~ lbody, data = brains)
library(gamlss.mx)
# mixture = "np":  nonparametric finite mixtures
br.2 <- gamlssNP(formula = lbrain ~ lbody, mixture = "np", K = 2, tol = 1, data = brains, family = NO, plot.opt = 0)
br.3 <- gamlssNP(formula = lbrain ~ lbody, mixture = "np", K = 3, tol = 1, data = brains, family = NO, plot.opt = 0)
br.4 <- gamlssNP(formula = lbrain ~ lbody, mixture = "np", K = 4, tol = 1, data = brains, family = NO, plot.opt = 0)
# ----------
GAIC(br.1, br.2, br.3, br.4)
GAIC(br.1, br.2, br.3, br.4, k = log(length(brains$body)))
# -->
# The model br.3 with 3 components (i.e. three parallel lines) is selected by both criteria.
# ----------
br.3
# -->
# Model br.3 can be presented as Y ~ NO(mu, sigma) where
# mu = -3.0715 + 0.750 * x:  with probability 0.107
# mu =  1.909 + 0.750 * x:  with probability 0.751
# mu =  3.482 + 0.750 * x:  with probability 0.141
# and sigma = exp(-0.9387) = 0.391
# ----------
# The estimated posterior (conditional) probabilities
head(br.3$post.prob[[1]])
# ----------
# residual plot of the finite mixture model
plot(br.3)
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.3, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
br.31 <- gamlssNP(formula = lbrain ~ lbody, sigma.fo = ~ MASS, mixture = "np", K = 3, tol = 1, data = brains, family = NO)
# mu intercept: different  +  mu slope: different  + sigma: same
br.32 <- gamlssNP(formula = lbrain ~ lbody, random = ~ lbody, sigma.fo = ~1, mixture = "np", K = 3, tol = 1, data = brains, family = NO)
# mu intercept: different  +  mu slope: different  + sigma: different
br.33 <- gamlssNP(formula = lbrain ~ lbody, random = ~ lbody, sigma.fo = ~MASS, mixture = "np", K = 3, tol = 1, data = brains, family = NO)
# ----------
GAIC(br.3, br.31, br.32, br.33)
GAIC(br.3, br.31, br.32, br.33, k = log(length(brains$lbody)))
# -->
# Model br.3 has the smallest SBC value.
# (Note that model br.32 has the smallest AIC value, however with so many parameters in the model and so few data points the model
# produces rather nonsensical results.)
# Note also that, in general, since model br.33 has components with no parameters in common it could also be fitted
# using gamlssMX().
# Again with only 27 observations any fitted model is too sensitive to starting values.
# ------------------------------------------------------------------------------
# Plot fitted values and weighted average for br.33 model
# ------------------------------------------------------------------------------
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.33, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
# ----------
# The weighted average for the (conditional) parameters mu for the K components for each observation
fitted(br.33, K = 0)
GAIC(br.3, br.31, br.32, br.33)
GAIC(br.3, br.31, br.32, br.33, k = log(length(brains$lbody)))
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.33, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.3, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
fitted(br.3, K = 0)
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.33, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
br.2 <- gamlssNP(formula = lbrain ~ lbody, mixture = "np", K = 2, tol = 1, data = brains, family = NO, plot.opt = 1)
br.3
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.3, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
par(mfrow = c(1,1))
with(brains, plot(lbody, lbrain,
pch = c(21, 22, 23)[max.col(br.3$post.prob[[1]])],
bg = c("red", "green3", "blue")[max.col(br.3$post.prob[[1]])]))
for(k in 0:3){
with(brains, lines(fitted(br.33, K = k)[order(lbody)] ~ lbody[order(lbody)],
lty = k+1, lwd = 2, col = c("black", "red", "green3", "blue")[k+1]))
}
legend("topleft", legend = c("Weighted Average", "Component 1", "Component 2", "Component 3"), pch = c(20, 21, 22, 23),
pt.bg = c("black", "red", "green3", "blue"), lty = 1:4, lwd = 2, col = c("black", "red", "green3", "blue"))
# ----------
# The weighted average for the (conditional) parameters mu for the K components for each observation
fitted(br.33, K = 0)
MASS
br.33 <- gamlssNP(formula = lbrain ~ lbody, random = ~ lbody, sigma.fo = ~MASS, mixture = "np", K = 3, tol = 1, data = brains, family = NO)
