# merge covariate names with missing indicator names
ELS.data <- cbind(ELS.data, missing.indicator[,propMissing > 0])
str(ELS.data)
load("Chapter2_ELS_data_example_career_academy.Rdata")
dim(ELS.data)
str(ELS.data)
# variable names starting with BY were measured in the base year (2002) of the Education Longtudinal Study (ELS)
# variable starting with F1 are composite variables created in 2004 but considered time invariant.
car::some(ELS.data)
# ----------
# To improve clarity, the indicator of participation in a career academy is recoded into the variable treat.
# BYS33K: the indicator of participation in career academy
table(ELS.data$BYS33K)
# create a treatment indicator
ELS.data$treat <- factor(ELS.data$BYS33K)
# ------------------------------------------------------------------------------
# Check missing cases
# ------------------------------------------------------------------------------
# examine the number of missing cases and its proportion
missing.indicator <- data.frame(is.na(ELS.data))
( propMissing <- apply(missing.indicator, 2, mean) )
sort(round(propMissing, digits = 3), decreasing = T)
# -->
# At maximum, 21.2% of cases are missing values for "bygpared" (Highest reported level of education among parents)
# many variables have missing cases.
# ------------------------------------------------------------------------------
# Create missing indicator
# ------------------------------------------------------------------------------
# create dummy missing value indicators
names(missing.indicator)[propMissing > 0] <- paste(names(ELS.data)[propMissing > 0], "NA", sep="")
names(missing.indicator)
# ----------
# convert dummy missing indicators from logical to numeric variables
for (var in 1:ncol(missing.indicator)) {
missing.indicator[,var] <- as.numeric(missing.indicator[,var]) }
# ----------
# merge covariate names with missing indicator names
ELS.data <- cbind(ELS.data, missing.indicator[,propMissing > 0])
str(ELS.data)
library(mice)
# impute separately treated and untreated groups
long.imputation <- c()
# quickpred: create matrix of zeros and ones where the rows indicate the variables to be imputed and the columns indicate the predictors selected for each variable.
# This allows the creation of an imputation model for each covariate, where the mincor = 0.1 argument specifies that a predictor is included only if
# its Pearson correlation with the covariate has an absolute value of at least 0.1, and the minpuc = 0.5 argument specifies that predictors should have
# at least 50 % of usable cases.
# With the exclude = c("STU_ID") argument, the student id variable is excluded.
# The mice function with the m = 5 argument obtains five multiple imputations by chained equations.
# The method = "pmm" argument specifies predictive mean matching (PMM) as the univariate imputation model,
# while the visitSequence = "monotone" argument specifies an imputing order from the variable with the least missing data to the variable with the most missing data.
# The imputed data sets are stored withing the long.imputation object.
# The complete function of the mice package allows extracting imputed data sets in a variety of formats: single imputed data sets, stacked imputed data sets in a long format,
# and a wide data set with imputed data sets side by side.
# Here the action = "long" argument is used to specify that the imputed data sets will be stacked in a long format.
# Although a different type of univariate imputation model could have been specified for each variable, in this example, the model specification
# is simplified by using PMM as the univariate imputation method for all variables.
# PMM can handle imputing both continuous and categorical variables. PMM is a 2-step process that first computes predicted values for the variable being imputed.
# Then, for each missing value, PMM randomly samples an observed value from a set of candidate donors that have predicted values close to the predicted values of the missing value.
# PMM has several advantages: (1) It does not require the specification of an explicit model for the distribution of missing values,
# (2) it guarantees that imputed values are always withing the range of observed values, and
# (3) it produces imputed missing values with similar distributions as the observed values, which is particularly usefule if the observed variable
# is nonnormally distributed.
for (group in 0:1) {
# creates a list of predictors of missing data with a mininum correlation of 0.1 and at least 50% of useful data
predictor.selection <- quickpred(subset(ELS.data, treat == group), mincor = 0.1, minpuc = 0.5, method = 'pearson', exclude = c("STU_ID"))
# impute variables by from least missing to most missing
# Using multiple imputation by chained equations
# with predictive mean matching as the univariate imputation method
imputation <- mice(subset(ELS.data, treat == group), m = 5, method = "pmm", visitSequence = "monotone", predictorMatrix = predictor.selection)
# extract stacked data files
long.imputation = rbind(long.imputation, complete(imputation, action = "long"))
}
names(ELS.data)
table(ELS.data$treat)
imputation1 <- subset(long.imputation, subset=.imp == 1)
head(imputation1)
library(mitools)
# this object was specifically designed to be analyzed with the survey package
allImputations <- imputationList(list(
subset(long.imputation, subset=.imp==1),
subset(long.imputation, subset=.imp==2),
subset(long.imputation, subset=.imp==3),
subset(long.imputation, subset=.imp==4),
subset(long.imputation, subset=.imp==5)))
allImputations
covariateNames <-
c("BYSTLNG2",#Sample member^s English fluency
"byplang",#Parent^s English fluency
"byfcomp", #Family composition
"bysibstr",# BY in-home sibling structure
"bysibhom", # BY number of in-home siblings
"bygnstat", # Generational status
"bypared", #Parents' highest level of education
"bygpared", #Highest reported level of education among parents^ parents
"BYSES1QU", #Quartile coding of SES1 variable
"bygrdrpt", #Number of grades repeated (K-10)
"byhomlit", #BY home literacy resources
"byriskfc", #Number of academic risk factors in 10th grade
"bystexp", # How far in school student thinks will get-composite
"F1SEX", #F1 sex-composite    Composite    Weights and Composites for F1
"F1RACE", #F1 student's race / ethnicity-composite	Composite	Weights and Composites for F1
"F1HOMLNG", # 	F1 student's native language-composite	Composite	Weights and Composites for F1
"BYS26", # 	High school program-student self-report	Questionnaire	BY Student Questionnaire
"BYS27D", # 	Education is important to get a job later	Questionnaire	BY Student Questionnaire
"BYS27I", # 	Parents expect success in school	Questionnaire	BY Student Questionnaire
"BYS28", # 	How much likes school	Questionnaire	BY Student Questionnaire
"BYS37", # 	Importance of good grades to student	Questionnaire	BY Student Questionnaire
"BYS38C", # 	How often goes to class without homework done	Questionnaire	BY Student Questionnaire
"BYS54A", # 	Importance of being successful in line work	Questionnaire	BY Student Questionnaire
"BYS54O", # 	Importance of getting good education	Questionnaire	BY Student Questionnaire
"BYS57", # 	Plans to continue education after high school	Questionnaire	BY Student Questionnaire
"BYS58", # 	Type of school plans to attend	Questionnaire	BY Student Questionnaire
"bysctrl", #public, private, other
"byurban", #urbanicity
"byregion", #geographic region of school
"BY10FLP") #grade 10 percent free or reduced lunch
# ------------------------------------------------------------------------------
# Add missing values indicator variable
# ------------------------------------------------------------------------------
# check whether any dummy missing indicators are redundant because variables have missing values for the same cases
# dummy indicators for covariates with less than 5% missing data were not included to avoid model estimation problems.
# If two dummy missing value indicators have correlations above 0.8, only one of them is added to the propensity score model
missingCorrelations <- cor(missing.indicator[, propMissing > 0.05])
missingCorrelations <- cor(missing.indicator[, propMissing > 0.05])
diag(missingCorrelations) <- 0
( maxCorrelations <- apply(missingCorrelations, 2, max) )
dummyNAnames <- names(maxCorrelations)[maxCorrelations < 0.8]
( maxCorrelationsHigh <- maxCorrelations[!duplicated(maxCorrelations)] )
( dummyNAnames <- c(dummyNAnames, names(maxCorrelationsHigh)[maxCorrelationsHigh >= 0.8]) )
( covariateNames <- c(covariateNames, dummyNAnames) )
psFormula <- paste(covariateNames, collapse="+")
psFormula <- formula(paste("treat~", psFormula, sep=""))
print(psFormula)
str(allImputations)
print(psFormula)
library(survey)
options(survey.lonely.psu = "adjust")
surveyDesign1 <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ bystuwt, data = imputation1, nest = T)
surveyDesignAll <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ bystuwt, data = allImputations, nest = T)
names(surveyDesign1)
names(surveyDesignAll)
psModel1 <- svyglm(psFormula, design = surveyDesign1, family = quasibinomial)
# fit a logistic regression model to all imputed datasets
psModelAll <- with(surveyDesignAll, svyglm(psFormula, family = quasibinomial))
pScores <-  fitted(psModel1)
imputation1$pScores <-  pScores
pScoresAll <- sapply(psModelAll, fitted)
# combine propensity scores across imputed datasets by taking the mean
pScoresMean <- apply(pScoresAll, 1, mean)
# add propensity score mean to imputed datasets
allImputations <- update(allImputations, pScores = pScoresMean)
library(party)
psFormula
# fit the tree to imputed data
myctree <- ctree(psFormula, data = imputation1)
myctree
# ----------
# obtain tiff image
tiff("ctree_for_treat.tif", res=600, compression = "lzw", height=6, width=15, units="in")
plot(myctree)
dev.off()
myctree
load("imputed_and_estimated_propensity_score")
getwd()
load("imputed_and_estimated_propensity_score")
load("imputed_and_estimated_propensity_score")
load("imputed_and_estimated_propensity_score.R")
setwd("C:\\Users\\kswad\\OneDrive\\デスクトップ\\技術力強化_統計解析\\51_解析スクリプト\\els_career_academy")
packages <- c("dplyr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# load results with imputed and estimated propensity scores
# ------------------------------------------------------------------------------
load("imputed_and_estimated_propensity_score.R")
load("imputed_and_estimated_propensity_score.Rdata")
with(ELS.data.imputed, by(pScores, treat, summary))
with(ELS.data.imputed, by(pScoresRf, treat, summary))
with(ELS.data.imputed, by(pScoresGBM, treat, summary))
by(ELS.data.imputed[,63:65], ELS.data.imputed$treat, summary)
round(by(ELS.data.imputed[,63:65], ELS.data.imputed$treat, summary), digits = 3)
by(ELS.data.imputed[,63:65], ELS.data.imputed$treat, summary)
tableCommonSupport = rbind(
summary(ELS.data.imputed[ELS.data.imputed$treat == 1, 63:65]),
summary(ELS.data.imputed[ELS.data.imputed$treat == 0, 63:65]))
rownames(tableCommonSupport) = c(rep("Treated",6), rep("Control",6))
tableCommonSupport
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1]))),
mean(as.numeric(pScoresRf[treat==0] < min(pScoresRf[treat==1]))),
mean(as.numeric(pScoresGBM[treat==0] < min(pScoresGBM[treat==1])))))
ELS.data.imputed$treat = factor(ELS.data.imputed$treat)
# ----------
library(lattice)
lattice.options(default.theme = standard.theme(color = FALSE))
bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE)
bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by logistic regression")
bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by logistic regression")
bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by random forest")
bwplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by GBM")
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression")
bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by random forest")
bwplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", main = "by GBM")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression")
boxplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by random forest")
boxplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", main = "by GBM")
library(lattice)
lattice.options(default.theme = standard.theme(color = FALSE))
bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by logistic regression")
bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by random forest")
bwplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by GBM")
bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by random forest")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression", ylim = c(0, 1))
boxplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by random forest", ylim = c(0, 1))
boxplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", main = "by GBM", ylim = c(0, 1))
require(lattice)
lattice.options(default.theme = standard.theme(color = FALSE))
densityplot( ~pScores, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
densityplot( ~pScoresRf, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
densityplot( ~pScoresGBM, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
allImputationsStacked <- data.frame()
for (imp in 1:5) {
temp <- cbind(allImputations$imputations[[imp]], imputation = imp)
allImputationsStacked = rbind(allImputationsStacked, temp)
}
allImputationsStacked$treat <- factor(allImputationsStacked$treat, levels = c(0,1), labels=c("Untreated", "Treated"))
allImputationsStacked$imputation <- factor(allImputationsStacked$imputation, labels = paste("Imputation", 1:5))
# ----------
lattice.options(default.theme = standard.theme(color = FALSE))
densityplot(~ pScores | imputation, data = allImputationsStacked, plot.points = F, lwd = 2, groups = treat, xlab = "Propensity Scores", auto.key = TRUE)
bwplot(pScores ~ treat | imputation, data = allImputationsStacked, lwd = 2, ylab = "Propensity Scores", auto.key = TRUE)
# normalize the base year student sampling weight so that it sums to the sample size
ELS.data.imputed$bystuwt
ELS.data.imputed$bystuwt <- ELS.data.imputed$bystuwt / mean(ELS.data.imputed$bystuwt)
options(survey.lonely.psu = "adjust")
# define design object that describes the sample characteristics
# the variable psu identifies the primary sampling units (cluster ids)
# the variable STRATA_ID identifies the strata ids
# the variable bystuwt identifies the base-year sampling weights for
# respondents of the 2002 and 2004 waves (Base year and 1st follow-up)
surveyDesign <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ bystuwt, data = ELS.data.imputed, nest = T)
surveyDesign
table(ELS.data$BYS33K)
table(ELS.data$BYS33K)
prop.table(ELS.data$BYS33K)
prop.table(table(ELS.data$BYS33K))
dim(ELS.data)
surveyDesign <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ bystuwt, data = ELS.data.imputed, nest = T)
surveyDesign
( treatmentTable <- svymean(~ treat, surveyDesign) )
table(ELS.data.imputed$treat)
prop.table(table(ELS.data.impute$treat))
prop.table(table(ELS.data.imputed$treat))
( treatmentTable <- svymean(~ treat, surveyDesign) )
mean(ELS.data.imputed$bystuwt)
ELS.data.imputed$bystuwt
mean(ELS.data.imputed$bystuwt)
ELS.data$bystuwt
summary(ELS.data)
sum(ELS.data$bystuwt)
head(ELS.data)
psych::describe(ELS.data$bystuwt)
sum(ELS.data$byexpwt)
ELS.data$byexpwt
psych::describe(ELS.data$bystuwt)
sum(ELS.data$byschwt)
ELS.data$byschwt
names(ELS.data)
ELS.data.imputed$weightATT <- with(ELS.data.imputed,ifelse(treat == 1, 1, pScores / (1 - pScores)))
ELS.data.imputed$weightATTRf <- with(ELS.data.imputed,ifelse(treat == 1, 1, pScoresRf / ( 1 - pScoresRf)))
ELS.data.imputed$weightATTGBM <- with(ELS.data.imputed,ifelse(treat == 1, 1, pScoresGBM / (1 - pScoresGBM)))
with(ELS.data.imputed, by(weightATT, treat, summary))
with(ELS.data.imputed, by(weightATTRf, treat, summary))
with(ELS.data.imputed, by(weightATTGBM, treat, summary))
ELS.data.imputed$weightATE <- with(ELS.data.imputed, ifelse(treat == 1, 1 / pScores, 1 / (1 - pScores)))
ELS.data.imputed$weightATERf <- with(ELS.data.imputed,ifelse(treat== 1,  1 / pScoresRf, 1 / (1 - pScoresRf)))
ELS.data.imputed$weightATEGBM <- with(ELS.data.imputed,ifelse(treat == 1, 1 / pScoresGBM, 1 / (1 - pScoresGBM)))
with(ELS.data.imputed, by(weightATE, treat, summary))
with(ELS.data.imputed, by(weightATERf, treat, summary))
with(ELS.data.imputed, by(weightATEGBM, treat, summary))
library(mitools)
# add propensity score mean to imputed datasets
allImputations <- update(allImputations, weightATT = ifelse(treat == 1, 1, pScores /(1 - pScores)))
with(allImputations, by(weightATT, treat, summary))
# (3) weight truncation:
# check the 99th percentile weight:
with(ELS.data.imputed, quantile(weightATE, 0.99))
# check how many observatios will have truncated weights
with(ELS.data.imputed, sum(weightATE > quantile(weightATE, 0.99)))
# truncate weights at the 99th percentile:
ELS.data.imputed$weightATETruncated <- with(ELS.data.imputed, ifelse(weightATE > quantile(weightATE, 0.99),  quantile(weightATE, 0.99), weightATE))
# check truncated weights
with(ELS.data.imputed, by(weightATETruncated, treat, summary))
table(ELS.data$bystuwt)
unique(ELS.data$bystuwt)
head(ELS.data)
xtabs(~ STRAT_ID + bystuwt, data = ELS.data)
xtabs(~ STRAT_ID + bystuwt, data = ELS.data)
xtabs(~ STRAT_ID + SCH_ID + bystuwt, data = ELS.data)
unique(ELS.data$bystuwt)
length(unique(ELS.data$bystuwt))
with(ELS.data.imputed, quantile(weightATE, 0.99))
# check how many observatios will have truncated weights
with(ELS.data.imputed, sum(weightATE > quantile(weightATE, 0.99)))
# truncate weights at the 99th percentile:
ELS.data.imputed$weightATETruncated <- with(ELS.data.imputed, ifelse(weightATE > quantile(weightATE, 0.99),  quantile(weightATE, 0.99), weightATE))
with(ELS.data.imputed, by(weightATETruncated, treat, summary))
ELS.data.imputed$C <- with(ELS.data.imputed, ifelse(treat == 1, pScores, 1 - pScores))
surveyDesign <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ bystuwt, data = ELS.data.imputed, nest = T) #recreate survey design
( constants <- svyby(~ C, by = ~ treat, design = surveyDesign, FUN = svymean) )
ELS.data.imputed$stabilizedWeightATE <- ifelse(ELS.data.imputed$treat == 1, constants[1,2] / ELS.data.imputed$C,
constants[2,2] / ELS.data.imputed$C)
# check stabilized weigts for extremeness
with(ELS.data.imputed, by(stabilizedWeightATE, treat, summary))
graphics.off()
par(mfrow = c(1,1))
with(ELS.data.imputed, hist(weightATE[treat==1], density = 10, angle = 45, main="Propensity Score Weights for the ATE", xlab="Shaded = IPTW | Gray = Stabilized IPTW") )
with(ELS.data.imputed, hist(stabilizedWeightATE[treat==1], col=gray(0.4,0.25), add=T) )
ELS.data.imputed$finalWeightBY <- with(ELS.data.imputed, bystuwt * weightATT)
library(twang)
balanceTable <- bal.stat(ELS.data.imputed, vars = covariateNames, treat.var = "treat",
w.all = ELS.data.imputed$finalWeightBY, get.ks = F, sampw = ELS.data.imputed$bystuwt, estimand = "ATT", multinom = F)
balanceTable <- balanceTable$results
head(balanceTable)
balanceTable2 <- bal.stat(ELS.data.imputed, vars = covariateNames, treat.var = "treat",
w.all = ELS.data.imputed$finalWeightBY, get.ks = F, sampw = ELS.data.imputed$bystuwt, estimand = "ATE", multinom = F)
balanceTable2 <- balanceTable2$results
head(balanceTable2)
head(balanceTable)
balanceSummarizer <- function(data, samplingWeight, PSWeight, treatment, effect, covariateNames) {
finalWeight <- data[, samplingWeight] * data[, PSWeight]
# evaluate covariate balance for ATT
require(twang)
balance.table <- bal.stat(data, vars= covariateNames, treat.var = treatment,
w.all = finalWeight, get.ks = F, sampw = data[, samplingWeight], estimand = effect, multinom = F)
balance.table <- balance.table$results
# calculate variance ratio
balance.table$varRatio <- with(balance.table, tx.sd^2/ct.sd^2)
# summarize the covariate balance quality
return(rbind(
std.eff.sz = summary(abs(balance.table$std.eff.sz)), #standardized effect sizes
varRatio = summary(balance.table$varRatio) )) # variance ratios
#close function
}
Table3.1 <- rbind(
logistic = balanceSummarizer(data=ELS.data.imputed, PSWeight="weightATT", effect="ATT",
samplingWeight="bystuwt",  treatment="treat",covariateNames=covariateNames),
RF = balanceSummarizer(data=ELS.data.imputed, PSWeight="weightATTRf", effect="ATT",
samplingWeight="bystuwt",  treatment="treat",covariateNames=covariateNames),
GBM = balanceSummarizer(data=ELS.data.imputed, PSWeight="weightATTGBM",effect="ATT",
samplingWeight="bystuwt",  treatment="treat",covariateNames=covariateNames))
Table3.1 <- data.frame(round(Table3.1, digits = 4), index=rep(c("std.eff.sz","varRatio"), 3), method = rep(c("Logistic","RF","GBM"), each=2))
# ----------
# Summary of absolute standardized effect sizes with weights for the ATT
# --> Absolute standardized effect sizes below 0.1 standard deviations can be considered to indicate adequate covariate balance,
# but differences below 0.25 standard deviations could be acceptable if additional regression adjustment is performed.
# Weights from propensity scores estimated with logistic regression provided the best covariate balance among the methods used to
# estimate propensity scores and met the criteria for adequate balance for all covariates.
head(Table3.1)
ELS.data.imputed$finalWeight2006 <- with(ELS.data.imputed, bystuwt * weightATT)
# normalize the base-year to second folow-up (2006) final weight
ELS.data.imputed$finalWeight2006 <- ELS.data.imputed$finalWeight2006 / mean(ELS.data.imputed$finalWeight2006)
# check distribution of final weights
summary(ELS.data.imputed$finalWeight2006)
surveyDesign2006 <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ finalWeight2006, data = ELS.data.imputed, nest = T)
summary(ELS.data.imputed$finalWeight2006)
surveyDesign2006 <- svydesign(ids = ~ psu, strata = ~ STRAT_ID, weights = ~ finalWeight2006, data = ELS.data.imputed, nest = T)
surveyDesign2006Boot <- as.svrepdesign(surveyDesign2006, type = c("bootstrap"), replicates = 1000)
outcomeModel2006 <- svyglm(F2ERN5P2 ~ treat, surveyDesign2006)
summary(outcomeModel2006)
( weightedMeans <- svyby(formula = ~ F2ERN5P2, by = ~ treat, design = surveyDesign2006Boot, FUN = svymean, covmat = TRUE) )
( ATT2006 <- svycontrast(weightedMeans, contrasts = c(-1, 1)) )
( weightedVars <- svyby(formula = ~ F2ERN5P2, by = ~ treat, design = surveyDesign2006Boot, FUN = svyvar, covmat = TRUE) )
( weightedVars <- svyby(formula = ~ F2ERN5P2, by = ~ treat, design = surveyDesign2006Boot, FUN = svyvar, covmat = TRUE) )
( ATT2006 <- svycontrast(weightedMeans, contrasts = c(-1, 1)) )
setwd("C:\\Users\\kswad\\OneDrive\\デスクトップ\\技術力強化_統計解析\\51_解析スクリプト\\els_career_academy")
packages <- c("dplyr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# load results with imputed and estimated propensity scores
# ------------------------------------------------------------------------------
load("imputed_and_estimated_propensity_score.Rdata")
# ------------------------------------------------------------------------------
# Summary statistics of propensity scores
# estimated with the first imuputed dataset
# ------------------------------------------------------------------------------
# logistic regression
with(ELS.data.imputed, by(pScores, treat, summary))
# random forest with conditional inference scheme
with(ELS.data.imputed, by(pScoresRf, treat, summary))
# GBM
with(ELS.data.imputed, by(pScoresGBM, treat, summary))
# for all models
by(ELS.data.imputed[,63:65], ELS.data.imputed$treat, summary)
tableCommonSupport = rbind(
summary(ELS.data.imputed[ELS.data.imputed$treat == 1, 63:65]),
summary(ELS.data.imputed[ELS.data.imputed$treat == 0, 63:65]))
rownames(tableCommonSupport) = c(rep("Treated",6), rep("Control",6))
tableCommonSupport
# write.csv(tableCommonSupport, file="Table_common_support.csv")
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
# obtain proportions of treated cases above maximum control cases
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1]))),
mean(as.numeric(pScoresRf[treat==0] < min(pScoresRf[treat==1]))),
mean(as.numeric(pScoresGBM[treat==0] < min(pScoresGBM[treat==1])))))
ELS.data.imputed$treat = factor(ELS.data.imputed$treat)
# ----------
library(lattice)
# lattice.options(default.theme = standard.theme(color = FALSE))
# bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by logistic regression")
# bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by random forest")
# bwplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by GBM")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression", ylim = c(0, 1))
boxplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by random forest", ylim = c(0, 1))
boxplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", main = "by GBM", ylim = c(0, 1))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
# obtain proportions of treated cases above maximum control cases
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
# obtain proportions of treated cases above maximum control cases
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1])))))
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==1] > max(pScores[treat==0]))),
mean(as.numeric(pScoresRf[treat==1] > max(pScoresRf[treat==0]))),
mean(as.numeric(pScoresGBM[treat==1] > max(pScoresGBM[treat==0])))))
# obtain proportions of control cases below minimum treated cases
with(ELS.data.imputed, 100*c(
mean(as.numeric(pScores[treat==0] < min(pScores[treat==1]))),
mean(as.numeric(pScoresRf[treat==0] < min(pScoresRf[treat==1]))),
mean(as.numeric(pScoresGBM[treat==0] < min(pScoresGBM[treat==1])))))
ELS.data.imputed$treat = factor(ELS.data.imputed$treat)
# ----------
library(lattice)
# lattice.options(default.theme = standard.theme(color = FALSE))
# bwplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by logistic regression")
# bwplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by random forest")
# bwplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", auto.key = TRUE, main = "by GBM")
par(mfrow = c(2,2))
boxplot(pScores ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by logistic regression", ylim = c(0, 1))
boxplot(pScoresRf ~ treat, data = ELS.data.imputed, ylab = "Propensity Scores", xlab = "Treatment", main = "by random forest", ylim = c(0, 1))
boxplot(pScoresGBM ~ treat, data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment", main = "by GBM", ylim = c(0, 1))
require(lattice)
lattice.options(default.theme = standard.theme(color = FALSE))
densityplot( ~pScores, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
densityplot( ~pScoresRf, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
densityplot( ~pScoresGBM, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
require(lattice)
lattice.options(default.theme = standard.theme(color = FALSE))
densityplot( ~pScores, groups=treat, plot.points=F, xlim=c(0,1), lwd=2,
data = ELS.data.imputed,  ylab = "Propensity Scores", xlab = "Treatment",auto.key = TRUE)
performance(prediction(ELS.data.imputed$pScores, ELS.data.imputed$treat), measure = "auc")@y.values[[1]]
library(ROCR)
# AUC
performance(prediction(ELS.data.imputed$pScores, ELS.data.imputed$treat), measure = "auc")@y.values[[1]]
par(mfrow = c(1,1))
perf <- performance(prediction(matched_data$distance, matched_data$treatment), "tpr", "fpr")
plot(perf, col = "blue")
abline(a = 0, b = 1)
par(mfrow = c(1,1))
perf <- performance(prediction(ELS.data.imputed$pScores, ELS.data.imputed$treat), "tpr", "fpr")
plot(perf, col = "blue")
abline(a = 0, b = 1)
# AUC
performance(prediction(ELS.data.imputed$pScores, ELS.data.imputed$treat), measure = "auc")@y.values[[1]]
