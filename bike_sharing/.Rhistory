cvCtrl <- trainControl(method="repeatedcv", repeats=3)
model <- caret::train(bike.train.x, bike$cnt, method = 'neuralnet', algorithm = 'backprop', learningrate = 0.25, hidden = 3, preProcess = c("center", "scale"), trControl=cvCtrl, trace = TRUE, linout = TRUE)
cvCtrl <- trainControl(method="repeatedcv", repeats=3)
model <- caret::train(bike.train.x, bike$cnt, method = 'neuralnet', algorithm = 'backprop', learningrate = 0.25, hidden = 3, preProcess = c("center", "scale"), trControl=cvCtrl, trace = TRUE, linout = 1)
cvCtrl <- trainControl(method="repeatedcv", repeats=3)
model <- caret::train(bike.train.x, bike$cnt, method = 'neuralnet', algorithm = 'backprop', learningrate = 0.25, hidden = 2, preProcess = c("center", "scale"), trControl=cvCtrl, trace = TRUE, linout = TRUE)
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3)
model <- caret::train(bike.train.x, bike$cnt, method = 'neuralnet', trControl=cvCtrl, trace = TRUE, linout = TRUE)
help(neuralnet)
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3)
model <- caret::train(bike.train.x, bike$cnt, method = 'neuralnet', algorithm = 'backprop', learningrate = 0.25, hidden = c(5, 5), preProcess = c("center", "scale"), trControl=cvCtrl, trace = TRUE, linout = TRUE)
model <- neuralnet(bike.train.x, bike$cnt, learningrate = 0.05, hidden = c(5, 5))
library(neuralnet)
model <- neuralnet(bike.train.x, bike$cnt, learningrate = 0.05, hidden = c(5, 5))
names(bike.train.x)
model <- neuralnet(cnt ~ ., data = bike, learningrate = 0.05, hidden = c(5, 5))
model <- neuralnet(cnt ~ ., data = bike, learningrate = 0.05, hidden = c(3, 3))
names(bike)
model <- neuralnet(cnt ~ ., data = bike)
str(bike)
model <- neuralnet(cnt ~ ., data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- neuralnet(cnt ~ day_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
predictor = Predictor$new(model, data = bike)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model))
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed + as.numeric(weekday), data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed + as.numeric(weekday), data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- caret::train(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- caret::train(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, method = "neuralnet", learningrate = 0.05, hidden = c(3, 3), linear.out = TRUE)
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
model
library("iml")
predictor = Predictor$new(model, data = bike)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model))
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
avg.prediction = mean(predict(model, newdata = bike))
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
# ----------
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike))
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
predictor = Predictor$new(model, NEWdata = bike)
predictor = Predictor$new(model, newdata = bike)
predictor = Predictor$new(model, data = bike)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike))
avg.prediction
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
diff.prediction
library(neuralnet)
model <- neuralnet(cnt ~ days_since_2011 + temp + hum + windspeed, data = bike, learningrate = 0.1, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike))
actual.prediction = predict(model, newdata = bike[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
help(rapply)
bike_s <- rapply(bike, f = as.numeric, classes = "factor", how = c("replace"))
bike_s <- rapply(bike, f = as.numeric, classes = "factor", how = c("replace"))
str(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.1, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
care::some(bike_s)
car::some(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.1, hidden = c(10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.25, hidden = c(10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.25, hidden = c(10), linout = TRUE)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.25, hidden = c(10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
( diff.prediction = actual.prediction - avg.prediction )
bike_s <- apply(bike_s, FUN = scale, 2)
str(bike_s)
bike_s
head(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.25, hidden = c(10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
( diff.prediction = actual.prediction - avg.prediction )
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
str(bike_s)
bike_s <- rapply(bike, f = as.numeric, classes = "factor", how = c("replace"))
bike_s <- data.frame(apply(bike_s, FUN = scale, 2))
str(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.25, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
( diff.prediction = actual.prediction - avg.prediction )
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
( diff.prediction = actual.prediction - avg.prediction )
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
bike_s <- model.matrix(cnt ~ ., data = bike)
bike_s
head(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
model <- neuralnet(bike$cnt ~ bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
bike_s <- model.matrix(~ ., data = bike)
model <- neuralnet(cnt ~ ., learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
bike_s
head(bike_s)
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
model.list$variables
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
bike_s <- rapply(bike, f = as.numeric, classes = "factor", how = c("replace"))
head(bike_s)
bike_s <- apply(bike_s, FUN = scale, 2) %>% data.frame()
model <- neuralnet(cnt ~ ., data = bike_s, learningrate = 0.05, hidden = c(10, 10), linear.out = TRUE)
library(iml)
predictor = Predictor$new(model, data = bike_s)
instance_indices = c(295, 285)
avg.prediction = mean(predict(model, newdata = bike_s))
actual.prediction = predict(model, newdata = bike_s[instance_indices[2],])
( diff.prediction = actual.prediction - avg.prediction )
x.interest = bike_s[instance_indices[2],]
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
scale_x_discrete("")
task = makeRegrTask(data = bike, target = "cnt")
learner = makeLearner('regr.nnet')
library(mlr)
library(iml)
task = makeRegrTask(data = bike, target = "cnt")
# We fit a support vector machine model to predict the number of rented bikes, given weather conditions and calendar information.
learner = makeLearner('regr.nnet')
mod = mlr::train(learner, task)
predictor = Predictor$new(mod, data = bike[-which(names(bike) == "cnt")], y = bike$cnt)
importance = FeatureImp$new(predictor, loss = 'mae')
imp.dat = importance$results
# ----------
# The importance for each of the features in predicting bike counts with a support vector machine.
# The most important feature was temp, the least important was holiday.
best = which(imp.dat$importance == max(imp.dat$importance))
worst = which(imp.dat$importance == min(imp.dat$importance))
plot(importance) + scale_y_discrete("")
learner = makeLearner('regr.neuralnet')
learner = makeLearner('regr.nnet')
help(makeLearner)
learner = makeLearner('regr.h2o.deeplearning')
install.packages("h2o", dep = T)
learner = makeLearner('regr.h2o.deeplearning')
learner = makeLearner('regr.h2o.deeplearning',
par.vals = list(
hidden = c(10, 10),
nesterov_accelerated_gradient = TRUE,
categorical_encoding = "Auto"))
learner = makeLearner('regr.h2o.deeplearning',
par.vals = list(
classification = "FALSE",
hidden = c(10, 10),
nesterov_accelerated_gradient = TRUE,
categorical_encoding = "Auto"))
learner = makeLearner('regr.h2o.deeplearning',
par.vals = list(
hidden = c(10, 10),
nesterov_accelerated_gradient = TRUE))
mod = mlr::train(learner, task)
predictor = Predictor$new(mod, data = bike[-which(names(bike) == "cnt")], y = bike$cnt)
importance = FeatureImp$new(predictor, loss = 'mae')
imp.dat = importance$results
# ----------
# The importance for each of the features in predicting bike counts with a support vector machine.
# The most important feature was temp, the least important was holiday.
best = which(imp.dat$importance == max(imp.dat$importance))
worst = which(imp.dat$importance == min(imp.dat$importance))
plot(importance) + scale_y_discrete("")
# ----------
setwd("//media//kswada//MyFiles//R//bike_sharing")
packages <- c("dplyr", "caret", "lattice", "knitr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# data:  bike sharing  -->  create data by "01_bike_sharing_basics.R" scripts
# ------------------------------------------------------------------------------
bike.features.of.interest <- c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)
# ------------------------------------------------------------------------------
# set my theme for visualization
# ------------------------------------------------------------------------------
library("ggplot2")
library("viridis")
# define graphics theme
my_theme = function(legend.position='right'){
theme_bw() %+replace%
theme(legend.position=legend.position)
}
theme_set(my_theme())
default_color = "azure4"
setwd("//media//kswada//MyFiles//R//bike_sharing")
packages <- c("dplyr", "caret", "lattice", "knitr")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# data:  bike sharing  -->  create data by "01_bike_sharing_basics.R" scripts
# ------------------------------------------------------------------------------
bike.features.of.interest <- c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)
# ------------------------------------------------------------------------------
setwd("//media//kswada//MyFiles//R//bike_sharing")
packages <- c("dplyr", "caret", "lattice")
purrr::walk(packages, library, character.only = TRUE, warn.conflicts = FALSE)
# ------------------------------------------------------------------------------
# data:  bike sharing
#   - contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C.,
#     along with weather and seasonal information.
#   - The data was kindly made openly available by Capital-Bikeshare.
#     Fanaee-T and Gama (2013) added weather data and season information.
#     The goal is to predict how many bikes will be rented depending on the weather and the day.
#     The data can be downloaded from the UCI Machine Learning Repository.
#
#   - New features were added to the dataset and not all original features were used for the examples here.
#     Here is the list of features that were used:
#        - Count of bicycles including both casual and registered users. The count is used as the target in the regression task.
#        - The season, either spring, summer, fall or winter.
#        - Indicator whether the day was a holiday or not.
#        - The year, either 2011 or 2012.
#        - Number of days since the 01.01.2011 (the first day in the dataset). This feature was introduced to take account of the trend over time.
#        - Indicator whether the day was a working day or weekend.
#        - The weather situation on that day. One of: clear, few clouds, partly cloudy, cloudy
#        - mist + clouds, mist + broken clouds, mist + few clouds, mist
#        - light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds
#        - heavy rain + ice pallets + thunderstorm + mist, snow + mist
#        - Temperature in degrees Celsius.
#        - Relative humidity in percent (0 to 100).
#        - Wind speed in km per hour.
# ------------------------------------------------------------------------------
get.bike.data = function(data_dir){
bike = read.csv(sprintf('%s//day.csv', data_dir), stringsAsFactors = FALSE)
# See http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset
bike$weekday = factor(bike$weekday, levels=0:6, labels = c('SUN', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT'))
bike$holiday = factor(bike$holiday, levels = c(0,1), labels = c('NO HOLIDAY', 'HOLIDAY'))
bike$workingday = factor(bike$workingday, levels = c(0,1), labels = c('NO WORKING DAY', 'WORKING DAY'))
bike$season = factor(bike$season, levels = 1:4, labels = c('SPRING', 'SUMMER', 'FALL', 'WINTER'))
bike$weathersit = factor(bike$weathersit, levels = 1:3, labels = c('GOOD', 'MISTY', 'RAIN/SNOW/STORM'))
bike$mnth = factor(bike$mnth, levels = 1:12, labels = c('JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OKT', 'NOV', 'DEZ'))
bike$yr[bike$yr == 0] = 2011
bike$yr[bike$yr == 1] = 2012
bike$yr = factor(bike$yr)
bike$days_since_2011 = day_diff(bike$dteday, min(as.Date(bike$dteday)))
# denormalize weather features:
# temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
bike$temp = bike$temp * (39 - (-8)) + (-8)
# atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
bike$atemp = bike$atemp * (50 - (16)) + (16)
#windspeed: Normalized wind speed. The values are divided to 67 (max)
bike$windspeed = 67 * bike$windspeed
#hum: Normalized humidity. The values are divided to 100 (max)
bike$hum = 100 * bike$hum
dplyr::select(bike, -instant, -dteday, -registered, -casual, -atemp)
}
# ----------
# get.bike.task <- function(data_dir){
#  mlr::makeRegrTask(id='bike', data=get.bike.data(data_dir), target = 'cnt')
# }
# ----------
year_diff = function(date1, date2){
day_diff(date1, date2) / 365.25
}
day_diff = function(date1, date2){
as.numeric(difftime(as.Date(date1), as.Date(date2), units = 'days'))
}
# ----------
data_dir <- "//media//kswada//MyFiles//data//bike_sharing//"
bike <- get.bike.data(data_dir)
str(bike)
car::some(bike)
bike.features.of.interest <- c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)
library(tidyverse)
library(xgboost)
library(caret)
bike_2 = select(bike, -days_since_2011, -cnt, -yr)
bike_dmy = dummyVars(" ~ .", data = bike_2, fullRank=T)
bike_x = predict(bike_dmy, newdata = bike_2)
model_bike = xgboost(data = bike_x, nround = 10, objective="reg:linear", label= bike$cnt)
cat("Note: The functions `shap.score.rank, `shap_long_hd` and `plot.shap.summary` were
originally published at https://github.com/liuyanguu/Blogdown/blob/master/hugo-xmag/content/post/2018-10-05-shap-visualization-for-xgboost.Rmd
All the credits to the author.")
model_bike
sumamry(model_bike)
summary(model_bike)
plot(model_bike)
shap_result_bike = shap.score.rank(xgb_model = model_bike, X_train = bike_x, shap_approx = F)
# return matrix of shap score and mean ranked score list
shap.score.rank <- function(xgb_model = xgb_mod, shap_approx = TRUE,
X_train = mydata$train_mm){
require(xgboost)
require(data.table)
shap_contrib <- predict(xgb_model, X_train,
predcontrib = TRUE, approxcontrib = shap_approx)
shap_contrib <- as.data.table(shap_contrib)
shap_contrib[,BIAS:=NULL]
cat('make SHAP score by decreasing order\n\n')
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]
return(list(shap_score = shap_contrib,
mean_shap_score = (mean_shap_score)))
}
# ----------
# a function to standardize feature values into same range
std1 <- function(x){
return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}
# ----------
# prep shap data
shap.prep <- function(shap  = shap_result, X_train = mydata$train_mm, top_n){
require(ggforce)
# descending order
if (missing(top_n)) top_n <- dim(X_train)[2] # by default, use all features
if (!top_n%in%c(1:dim(X_train)[2])) stop('supply correct top_n')
require(data.table)
shap_score_sub <- as.data.table(shap$shap_score)
shap_score_sub <- shap_score_sub[, names(shap$mean_shap_score)[1:top_n], with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))
# feature values: the values in the original dataset
fv_sub <- as.data.table(X_train)[, names(shap$mean_shap_score)[1:top_n], with = F]
# standardize feature values
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
# SHAP value: value
# raw feature value: rfvalue;
# standarized: stdfvalue
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
shap_long2[, mean_value := mean(abs(value)), by = variable]
setkey(shap_long2, variable)
return(shap_long2)
}
# ----------
plot.shap.summary <- function(data_long){
x_bound <- max(abs(data_long$value))
require('ggforce') # for `geom_sina`
plot1 <- ggplot(data = data_long)+
coord_flip() +
# sina plot:
geom_sina(aes(x = variable, y = value, color = stdfvalue)) +
# print the mean absolute value:
geom_text(data = unique(data_long[, c("variable", "mean_value"), with = F]),
aes(x = variable, y=-Inf, label = sprintf("%.3f", mean_value)),
size = 3, alpha = 0.7,
hjust = -0.2,
fontface = "bold") + # bold
# # add a "SHAP" bar notation
# annotate("text", x = -Inf, y = -Inf, vjust = -0.2, hjust = 0, size = 3,
#          label = expression(group("|", bar(SHAP), "|"))) +
scale_color_gradient(low="#FFCC33", high="#6600CC",
breaks=c(0,1), labels=c("Low","High")) +
theme_bw() +
theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), # remove axis line
legend.position="bottom") +
geom_hline(yintercept = 0) + # the vertical line
scale_y_continuous(limits = c(-x_bound, x_bound)) +
# reverse the order of features
scale_x_discrete(limits = rev(levels(data_long$variable))
) +
labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value")
return(plot1)
}
# ----------
var_importance <- function(shap_result, top_n=10)
{
var_importance=tibble(var=names(shap_result$mean_shap_score), importance=shap_result$mean_shap_score)
var_importance=var_importance[1:top_n,]
ggplot(var_importance, aes(x=reorder(var,importance), y=importance)) +
geom_bar(stat = "identity") +
coord_flip() +
theme_light() +
theme(axis.title.y=element_blank())
}
# `shap_approx` comes from `approxcontrib` from xgboost documentation.
# Faster but less accurate if true. Read more: help(xgboost)
var_importance(shap_result_bike, top_n=10)
shap_result_bike = shap.score.rank(xgb_model = model_bike, X_train = bike_x, shap_approx = F)
var_importance(shap_result_bike, top_n = 10)
shap_long_bike = shap.prep(shap = shap_result_bike, X_train = bike_x, top_n = 10)
plot.shap.summary(data_long = shap_long_bike)
install.packages("ggforce",dep=T)
shap_result_bike = shap.score.rank(xgb_model = model_bike, X_train = bike_x, shap_approx = F)
var_importance(shap_result_bike, top_n = 10)
shap_long_bike = shap.prep(shap = shap_result_bike, X_train = bike_x, top_n = 10)
shap_result_bike
shap_long_bike
plot.shap.summary(data_long = shap_long_bike)
xgb.plot.shap(data = bike_x, # input data
model = model_bike, # xgboost model
features = names(shap_result_bike$mean_shap_score[1:10]), # only top 10 var
n_col = 3, # layout option
plot_loess = T # add red line to plot
)
ggplotgui::ggplot_shiny(bike)
help(xgb.plot.shap)
dim(bike_x)
shap_result_bike
dim(shap_result_bike)
shap_result_bike$score
dim(shap_result_bike$shap_score)
plot.shap.summary(data_long = shap_long_bike)
shap_long_bike
dim(shape_long_bike)
dim(shap_long_bike)
xgb.plot.shap(data = bike_x, # input data
model = model_bike, # xgboost model
features = names(shap_result_bike$mean_shap_score[1:10]), # only top 10 var
n_col = 3, # layout option
plot_loess = T # add red line to plot
)
